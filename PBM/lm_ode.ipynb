{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenberg-Marquardt mothod for ordinary differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to produce\n",
    "1. $\\varphi(\\mathbf{z})$ that produces $n\\times(p+1)$-dimension vector with input arguments of function, initial condition $\\mathbf{y}_0$, and parameters $\\mathbf{k}$.\n",
    "2. Levenberg-Marquardt parameter estimation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.integrate import solve_ivp, odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODE model and objective function\n",
    "For the intial value problem\n",
    "\\begin{equation}\n",
    "\\frac{\\text{d}\\mathbf{y}(t)}{\\text{d}t}=\\mathbf{f}(\\mathbf{y}(t),\\mathbf{k});\\hspace{10mm}\\mathbf{y}(t_0)=\\mathbf{y}_0\n",
    "\\end{equation}\n",
    "where $\\mathbf{k}$ is a parameter vector with $p$ elements, the objective function with the measurements $\\hat{\\mathbf{y}}_i$ ($i=1,\\cdots,N$) and the weighting matrix $\\mathbf{Q}_i$ is\n",
    "\\begin{equation}\n",
    "S(\\mathbf{k})=\\frac{1}{2}\\sum_{i=1}^N\\left[\\hat{\\mathbf{y}}_i-\\mathbf{y}(t_i,\\mathbf{k})\\right]^\\top\\mathbf{Q}_i\\left[\\hat{\\mathbf{y}}_i-\\mathbf{y}(t_i,\\mathbf{k})\\right]=\\frac{1}{2}\\sum_{i=1}^N\\mathbf{r}_i(\\mathbf{k})^\\top\\mathbf{Q}_i\\mathbf{r}_i(\\mathbf{k})\n",
    "\\end{equation}\n",
    "where $S(\\mathbf{k}):\\mathbb{R}^p\\to\\mathbb{R}$ and $\\mathbf{r}_i(\\mathbf{k})=\\hat{\\mathbf{y}}_i-\\mathbf{y}(t_i,\\mathbf{k})$ is the residuals. The gradient of the objective function is\n",
    "\\begin{equation}\n",
    "\\mathbf{g}=\\frac{\\partial S(\\mathbf{k})}{\\partial\\mathbf{k}}=-\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{r}_i\n",
    "\\end{equation}\n",
    "and the Hessian is\n",
    "\\begin{align}\n",
    "\\mathbf{H}&=\\frac{\\partial^2S(\\mathbf{k})}{\\partial\\mathbf{k}^2}=\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i-\\sum_{i=1}^N\\frac{\\partial\\mathbf{J^\\top}_i}{\\partial\\mathbf{k}}\\mathbf{Q}_i\\mathbf{r}_i\\\\\n",
    "&\\simeq\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\n",
    "\\end{align}\n",
    "where $\\mathbf{J}_i(\\mathbf{k})=\\frac{\\partial\\mathbf{y}(t_i,\\mathbf{k})}{\\partial\\mathbf{k}}$ is the Jacobian. The approximation neglecting final term is valid since it is often the case that the residuals $\\mathbf{r}_i$ are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Jacobian in ODE models\n",
    "In ODE models, the sensitivity matrix cannot be obtained by a simple differentiation. Instead, we can get differential equation for $\\mathbf{J}$.\n",
    "Differentiate both side of $ \\frac{\\text{d}\\mathbf{y}}{\\text{d}t}=\\mathbf{f}(\\mathbf{y}(t),\\mathbf{k})$ and apply the chain rule,\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\frac{\\text{d}\\mathbf{y}}{\\text{d}t}\\right)=\\frac{\\text{d}}{\\text{d}t}\\left(\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{k}}\\right)&=\\frac{\\partial}{\\partial\\mathbf{k}}\\mathbf{f}(\\mathbf{y}(t),\\mathbf{k})\\\\\n",
    "&=\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{k}}+\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{k}}\\frac{\\partial\\mathbf{k}}{\\partial\\mathbf{k}}\\\\\n",
    "&=\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{k}}+\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{k}}\n",
    "\\end{align}\n",
    "Hence,\n",
    "\\begin{equation*}\n",
    "\\frac{\\text{d}\\mathbf{J}(t)}{\\text{d}t}=\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\mathbf{J}(t)+\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{k}};\\hspace{10mm}\\mathbf{J}(t_0)=0\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruction of ODE system with Jacobian\n",
    "The Jacobian or the sensitivity matrix is\n",
    "\\begin{equation}\n",
    "\\mathbf{J}(t)=\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{k}}=\\left[\\frac{\\partial\\mathbf{y}}{\\partial k_1},\\cdots,\\frac{\\partial\\mathbf{y}}{\\partial k_p}\\right]=[\\mathbf{g}_1,\\cdots,\\mathbf{g}_p]\n",
    "\\end{equation}\n",
    "where $\\mathbf{g}_j$ represents $n$-dimensional vector which is the sensitivity coefficients of the state variables with respect to parameter $k_j$. Each of $\\mathbf{g}_j$ satisfies the differential equation for Jacobian such that\n",
    "\\begin{equation*}\n",
    "\\frac{\\text{d}\\mathbf{g}_j(t)}{\\text{d}t}=\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\mathbf{g}_j+\\frac{\\partial\\mathbf{f}}{\\partial k_j};\\hspace{10mm}\\mathbf{g}_j(t_0)=0;\\hspace{10mm}j=1,\\cdots,p\n",
    "\\end{equation*}\n",
    "We generate $n\\times(p+1)$-dimensional differential equations system\n",
    "\\begin{equation*}\n",
    "\\frac{d\\mathbf{z}}{dt}=\\varphi(\\mathbf{z})\n",
    "\\end{equation*}\n",
    "$\\mathbf{z}$ is $n\\times(p+1)$-dimensional vector\n",
    "\\begin{equation*}\n",
    "\\mathbf{z}=\\begin{bmatrix} \\mathbf{x}(t)\\\\\n",
    "                          \\frac{\\partial\\mathbf{y}}{\\partial k_1}\\\\\n",
    "                          \\vdots\\\\\n",
    "                          \\frac{\\partial\\mathbf{y}}{\\partial k_p}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} \\mathbf{y}(t)\\\\\n",
    "                 \\mathbf{g}_1(t)\\\\\n",
    "                 \\vdots\\\\\n",
    "                 \\mathbf{g}_p(t)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$\\mathbf{\\varphi}(\\mathbf{z})$ is $n\\times(p+1)$-dimensional vector function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\varphi}(\\mathbf{z})=\\begin{bmatrix}\n",
    "\\mathbf{f}(\\mathbf{y},\\mathbf{k})\\\\\n",
    "\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\mathbf{g}_1(t)+\\frac{\\partial\\mathbf{f}}{\\partial k_1}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{y}}\\mathbf{g}_p(t)+\\frac{\\partial\\mathbf{f}}{\\partial k_p}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "To get the Jacobian for all $t_i$, $\\varphi(\\mathbf{z}_i)$ should be solved for $t_i,~~i=1,2,\\cdots,N$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange of the solution\n",
    "The Jacobian $\\textbf{J}$ is obtained by integration of $\\varphi(z)$. Integration of $\\varphi(z)$ returns $n\\times(p+1)$ vector\n",
    "\\begin{equation*}\n",
    "\\textbf{z}=\\begin{bmatrix}\n",
    "\\textbf{y}\\\\\n",
    "\\textbf{g}_1\\\\\n",
    "\\textbf{g}_2\\\\\n",
    "\\vdots\\\\\n",
    "\\textbf{g}_p\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\\textbf{g}_j=\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial k_j}\\\\\n",
    "\\frac{\\partial y_2}{\\partial k_j}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial y_n}{\\partial k_j}\n",
    "\\end{bmatrix},\\hspace{20mm}j=1,\\cdots,p\n",
    "\\end{equation*}\n",
    "To compute the Hessian\n",
    "\\begin{equation*}\n",
    "\\mathbf{H}=\\sum_{i=1}^N\\mathbf{J}(t_i)^\\top\\mathbf{Q}_i\\mathbf{J}(t_i)\n",
    "\\end{equation*}\n",
    "the Jacobian for all measurement time $\\mathbf{J}(t_1),\\cdots,\\mathbf{J}(t_N)$, should be returned.\n",
    "The ODE solver for initial value problem returns $[n\\times(p+1)]\\times N$ matrix\n",
    "\\begin{equation*}\n",
    "Z=\\begin{bmatrix}\n",
    "\\mathbf{y}(t_1)&\\mathbf{y}(t_2)&\\cdots&\\mathbf{y}(t_N)\\\\\n",
    "\\mathbf{g}_1(t_1)&\\mathbf{g}_1(t_2)&\\cdots&\\mathbf{g}_1(t_N)\\\\\n",
    "\\vdots&&\\ddots&\\vdots\\\\\n",
    "\\mathbf{g}_p(t_1)&\\mathbf{g}_p(t_2)&\\cdots&\\mathbf{g}_p(t_N)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "This matrix would be refomulated for\n",
    "\\begin{equation*}\n",
    "\\textbf{Y}=\\begin{bmatrix}\n",
    "\\mathbf{y}(t_1)&\\mathbf{y}(t_2)&\\cdots&\\mathbf{y}(t_N)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{align*}\n",
    "\\mathcal{J}=&\\begin{bmatrix}\n",
    "\\mathbf{J}(t_1),\\mathbf{J}(t_2),\\cdots,\\mathbf{J}(t_N)\n",
    "\\end{bmatrix}\\\\\n",
    "=&\\left[\\begin{bmatrix}\n",
    "\\textbf{g}_1(t_1)&\\textbf{g}_2(t_1)&\\cdots&\\textbf{g}_p(t_1)\n",
    "\\end{bmatrix}\n",
    ",\\cdots,\\begin{bmatrix}\n",
    "\\textbf{g}_1(t_N)&\\textbf{g}_2(t_N)&\\cdots&\\textbf{g}_p(t_N)\n",
    "\\end{bmatrix}\\right].\n",
    "\\end{align*}\n",
    "Note that $\\mathbf{Y}$ is an $n\\times N$ matrix and $\\mathcal{J}$ is an $n\\times p\\times N$ tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration caveats\n",
    "For ODE models, it is unnecessary to integrate the ODEs for the entire time domain $[t_1,t_N]$. Once the objective function $S(\\mathbf{k+h})$ becomes greater than $S(\\mathbf{k})$, the integration should be stopped and retried with another $\\mathbf{h}$ or with appropriate line search. This can prevent not only the unnecessary computation but also numerical instability in integration ODEs with the parameters far from the solution which can induce the computer overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration(ode,z0,k,t,n,p):\n",
    "    def dzdt(t,z):\n",
    "        return phi_ode(ode,z,k,n,p)\n",
    "    solution = solve_ivp(dzdt,[t[0],t[-1]],z0,method='Radau',t_eval=t)\n",
    "    return solution.y, solution.success\n",
    "    \n",
    "\n",
    "def integ_onestep(ode,z0,k,t,n,p):\n",
    "    def dzdt(t,z):\n",
    "        return phi_ode(ode,z,k,n,p)\n",
    "    solution = solve_ivp(dzdt,[t[0],t[-1]],z0,method='Radau',t_eval=t)\n",
    "    z = solution.y[:,-1]\n",
    "    return z, solution.success\n",
    "\n",
    "def phi_ode(ode,z,k,n,p):\n",
    "    y = z[0:n]\n",
    "    J = z[n:].reshape((p,n)).transpose()\n",
    "    phiz = np.empty(n*(p+1))\n",
    "    dfdy = dfdy_ode(ode,y,k,n)\n",
    "    dfdk = dfdk_ode(ode,y,k,n,p)\n",
    "    dJdt = dfdy@J+dfdk\n",
    "    phiz[0:n] = ode(y,k)\n",
    "    phiz[n:] = dJdt.transpose().flatten()\n",
    "    return phiz\n",
    "\n",
    "def dfdy_ode(ode,y,k,n):\n",
    "    h = 1e-8\n",
    "    y = y.astype(np.float)\n",
    "    if np.isscalar(y):\n",
    "        dfdy = (ode(y+h,k)-ode(y-h,k))/(2*h)\n",
    "        return dfdy\n",
    "    else:\n",
    "        dfdy = np.empty((n,n))\n",
    "        for i in range(n):\n",
    "            yr = y.copy()\n",
    "            yl = y.copy()\n",
    "            yr[i] += h\n",
    "            yl[i] -= h\n",
    "            dfdy[i] = (ode(yr,k)-ode(yl,k))/(2*h)\n",
    "        return dfdy.transpose()\n",
    "    return\n",
    "\n",
    "def dfdk_ode(ode,y,k,n,p):\n",
    "    h = 1e-8\n",
    "    if p == 1:\n",
    "        dfdk = (ode(y,k+h)-ode(y,k-h))/(2*h)\n",
    "        return dfdk.reshape(n,1)\n",
    "    else:\n",
    "        k = k.astype(np.float)\n",
    "        dfdk = np.empty((p,n))\n",
    "        for i in range(p):\n",
    "            kr = k.copy()\n",
    "            kl = k.copy()\n",
    "            kr[i] += h\n",
    "            kl[i] -= h\n",
    "            dfdk[i] = (ode(y,kr)-ode(y,kl))/(2*h)\n",
    "        return dfdk.transpose()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talyor expansion of scalar function of several variables\n",
    "The line passes $\\mathbf{x}_0$ along the direction $\\mathbf{s}$ is the set of points satisfying $\\mathbf{x}(\\alpha)=\\mathbf{x}_0+\\alpha\\mathbf{s}$ for all $\\alpha$. For a scalar value function $f(\\mathbf{x})$, the directional derivative of $f$ at $\\mathbf{x}_0$ in the direction of $\\mathbf{s}$ is,\n",
    "\\begin{equation}\n",
    "D_\\mathbf{s}f(\\mathbf{x}_0)=\\lim_{\\alpha\\to0}\\frac{f(\\mathbf{x}_0+\\alpha\\mathbf{s})-f(\\mathbf{x}_0)}{\\alpha}=\\frac{\\text{d}f(\\textbf{x})}{\\text{d}\\alpha}\\bigg|_{\\textbf{x}=\\textbf{x}_0}\n",
    "\\end{equation}\n",
    "By the chain rule,\n",
    "\\begin{equation}\n",
    "\\frac{\\text{d}}{\\text{d}\\alpha}=\\sum_i\\frac{\\partial}{\\partial x_i}\\frac{\\text{d}x_i(\\alpha)}{\\text{d}\\alpha}=\\sum_is_i\\frac{\\partial}{\\partial x_i}=\\mathbf{s^\\top \\nabla}\n",
    "\\end{equation}\n",
    "so the slope of $f$ along any line $\\mathbf{x}(\\alpha)$ is\n",
    "\\begin{equation}\n",
    "\\frac{\\text{d}f}{\\text{d}\\alpha}=\\mathbf{s}^\\top\\nabla f=\\nabla f^\\top\\mathbf{s}\n",
    "\\end{equation}\n",
    "and the curvature along the line is\n",
    "\\begin{equation}\n",
    "\\frac{\\text{d}^2 f}{\\text{d}\\alpha^2}=\\frac{\\text{d}}{\\text{d}\\alpha}\\frac{\\text{d}f}{\\text{d}\\alpha}=\\mathbf{s}^\\top\\nabla(\\nabla f^\\top\\mathbf{s})=\\mathbf{s}^\\top\\nabla^2f\\mathbf{s}.\n",
    "\\end{equation}.\n",
    "The Taylor expansion of multiple variables is\n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}_0+\\alpha\\mathbf{s})=f(\\mathbf{x}_0)+\\alpha\\mathbf{s}^\\top\\nabla f(\\mathbf{x}_0)+\\frac{1}{2}\\alpha^2\\mathbf{s}^\\top\\left[\\nabla^2f(\\mathbf{x}_0)\\right]\\mathbf{s}+\\cdots\n",
    "\\end{equation}\n",
    "or\n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}_0+\\mathbf{h})=f(\\mathbf{x}_0)+\\mathbf{h}^\\top\\nabla f(\\mathbf{x}_0)+\\frac{1}{2}\\mathbf{h}^\\top\\left[\\nabla^2f(\\mathbf{x}_0)\\right]\\mathbf{h}+\\cdots.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton method\n",
    "The Taylor expansion of $\\mathbf{r}_i$ respect to $\\mathbf{h}$ is\n",
    "$$\\mathbf{r}_i(\\mathbf{k+h})=\\mathbf{r}_i(\\mathbf{k})+\\frac{\\partial\\mathbf{r}_i(\\mathbf{k})}{\\partial\\mathbf{k}}\\mathbf{h}+\\mathcal{O}(\\mathbf{h^\\top h})=\\mathbf{r}_i(\\mathbf{k})-\\mathbf{J}_i(\\mathbf{k})\\mathbf{h}+\\mathcal{O}(\\mathbf{h^\\top h}).$$  \n",
    "In Gauss-Newton (GN) method, $\\mathbf{r}_i$ is approximated to a linear model\n",
    "\\begin{equation}\n",
    "\\mathbf{l}_i=\\mathbf{r}_i(\\mathbf{k})-\\mathbf{J}_i(\\mathbf{k})\\mathbf{h}\n",
    "\\end{equation}\n",
    "Inserting $\\mathbf{l}_i$ to the objective function yields\n",
    "\\begin{align}\n",
    "S(\\mathbf{k+h})&\\simeq L(\\mathbf{h})=\\frac{1}{2}\\sum_{i=1}^N\\mathbf{l}_i^\\top(\\mathbf{h})\\mathbf{Q}_i\\mathbf{l}_i(\\mathbf{h})\\\\\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^N\\left[\\mathbf{r}_i\\mathbf{(k)}^\\top\\mathbf{Q}_i\\mathbf{r}_i\\mathbf{(k)}-\\mathbf{r}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\\mathbf{(k)h}-\\mathbf{h}^\\top\\mathbf{J}_i\\mathbf{(k)}^\\top\\mathbf{Q}_i\\mathbf{r}_i(\\mathbf{k})+\\mathbf{h}^\\top\\mathbf{J}_i\\mathbf{(k)}^\\top\\mathbf{Q}_i\\mathbf{J}_i\\mathbf{(k)}\\mathbf{h}\\right]\\\\\n",
    "    &=S(\\mathbf{k})-\\sum_{i=1}^N\\mathbf{h^\\top J}_i^\\top\\mathbf{Q}_i \\mathbf{r}_i+\\frac{1}{2}\\sum_{i=1}^N\\mathbf{h^\\top J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\\mathbf{h}\n",
    "\\end{align}\n",
    "since $\\mathbf{r}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\\mathbf{(k)h}=\\mathbf{h}^\\top\\mathbf{J}_i\\mathbf{(k)}^\\top\\mathbf{Q}_i\\mathbf{r}_i(\\mathbf{k})$.  \n",
    "The gradient and Hessian of $L$ are\n",
    "\\begin{equation}\n",
    "\\mathbf{L'(h)}=\\sum_{i=1}^N\\left[-\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{r}_i+\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\\mathbf{h}\\right]\\hspace{20mm}\\mathbf{L''(h)}=\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\n",
    "\\end{equation}\n",
    "The Hessian is independent of $\\mathbf{h}$, symmetric and positive definite if $\\mathbf{J}$ has full rank. Hence $L$ is minimum when $\\mathbf{L'(h)}$ is zero vector. The step $\\mathbf{h}$ can be calculated by,\n",
    "\\begin{equation}\n",
    "\\left[\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i\\right]\\mathbf{h}=\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{r}_i=\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\left(\\hat{\\mathbf{y}}_i-\\mathbf{y}(t_i,\\mathbf{k})\\right)\n",
    "\\end{equation}\n",
    "so that\n",
    "\\begin{equation}\n",
    "\\mathbf{Hh=-g}\n",
    "\\end{equation}\n",
    "GN method is not quadratic convergent if $\\mathbf{r}_i$ is not zero around the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping criteria\n",
    "When the algorithm can be expected to converge rapidly, the criteria\n",
    "\\begin{equation}\n",
    "S^{(k)}-S^{(k+1)}\\leq\\varepsilon\n",
    "\\end{equation}\n",
    "works well. For algorithms converge less rapidly, a test based on\n",
    "\\begin{equation}\n",
    "\\Vert\\mathbf{g}^{(k)}\\Vert\\leq\\varepsilon\n",
    "\\end{equation}\n",
    "is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt method\n",
    "In Levenberg-Marquardt (LM) method, GN method is used with a damping term,\n",
    "\\begin{equation*}\n",
    "\\left[\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{J}_i+\\mu\\mathbf{I}\\right]\\mathbf{h}_\\text{lm}=\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{r}_i\n",
    "\\end{equation*}\n",
    "For large $\\mu$, $\\mathbf{h}_\\text{lm}\\simeq-\\frac{1}{\\mu}\\mathbf{L'(0)}$ is a short step in the steepest descent direction. This is good if the estimation is far from the solution. If $\\mu$ is very small, LM method is nearly GN method which is almost quadratic convergent if $S(\\mathbf{k})$ is close to zero.\n",
    "### Initial $\\mu$\n",
    "The initial value of $\\mu$ is maximum diagonal element of $\\mathbf{H}_0=\\sum\\mathbf{J}_i(\\mathbf{k}_0)^\\top\\mathbf{Q}_i\\mathbf{J}_i(\\mathbf{k}_0)$,\n",
    "\\begin{equation*}\n",
    "\\mu_0=\\tau\\cdot\\max[\\text{diagonal}(\\mathbf{H}_0)]\n",
    "\\end{equation*}\n",
    "where $\\tau$ is small such as $10^{-6}$ if $\\mathbf{k}_0$ is believed to be a good approximation or large such as $10^{-3}$ or $1$ otherwise.\n",
    "### Gain ratio\n",
    "The updating of $\\mu$ is controlled by the gain ratio\n",
    "\\begin{equation*}\n",
    "\\rho=\\frac{S(\\mathbf{k})-S(\\mathbf{k+h}_\\text{lm})}{L(\\mathbf{0})-L(\\mathbf{h}_\\text{lm})}\n",
    "\\end{equation*}\n",
    "The denominator is the gain predicted by the linear model,\n",
    "\\begin{align*}\n",
    "L(\\mathbf{0})-L(\\mathbf{h}_\\text{lm})=&\\mathbf{h}_\\text{lm}^\\top\\sum_{i=1}^N\\mathbf{J}_i\\mathbf{Q}_i\\mathbf{r}_i-\\frac{1}{2}\\mathbf{h}^\\top_\\text{lm}\\left[\\sum_{i=1}^N\\mathbf{J}_i\\top\\mathbf{Q}_i\\mathbf{J}_i\\right]\\mathbf{h}_\\text{lm}\\\\\n",
    "                   =&\\frac{1}{2}\\mathbf{h}_\\text{lm}^\\top\\left(2\\sum_{i=1}^N\\mathbf{J}_i\\mathbf{Q}_i\\mathbf{r}_i-\\left[\\sum_{i=1}^N\\mathbf{J}^\\top_i\\mathbf{Q}_i\\mathbf{J}_i+\\mu\\mathbf{I}\\right]\\mathbf{h}_\\text{lm}+\\mu\\mathbf{I}\\mathbf{h}_\\text{lm}\\right)\\\\\n",
    "                   =&\\frac{1}{2}\\mathbf{h}_\\text{lm}^\\top\\left(\\sum_{i=1}^N\\mathbf{J}_i^\\top\\mathbf{Q}_i\\mathbf{r}_i+\\mu\\mathbf{h}_\\text{lm}\\right)\\\\\n",
    "                   =&\\frac{1}{2}\\mathbf{h}_\\text{lm}^\\top\\left(-\\mathbf{g}+\\mu\\mathbf{h}_\\text{lm}\\right)\n",
    "\\end{align*}\n",
    "A large value of $\\rho$ indicates that $L(\\mathbf{h}_\\text{lm})$ is a good approximation of $S(\\mathbf{k+h}_\\text{lm})$ so $\\mu$ can be decreased so that LM is closer to GN method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Jacobian\n",
    "The Hessian $\\mathbf{H}$ might be ill-conditioned if the paramters differ by several order of magnitude. To overcome numerical problem, reduced Jacobian $\\mathbf{J}_\\text{R}$ is defined as\n",
    "$$\\mathbf{J}_\\text{R}=\\mathbf{JK}$$\n",
    "where $\\mathbf{K}=\\text{diag}(k_1,\\cdots,k_p)$. Then we can solve\n",
    "$$\\mathbf{H}_\\text{R}\\mathbf{h}_\\text{R}=-\\mathbf{g}_\\text{R}$$\n",
    "where $\\mathbf{H}_\\text{R}=\\mathbf{KHK}$ and $\\mathbf{g}_\\text{R}=\\mathbf{Kg}$.\n",
    "We can restore $\\textbf{h}$ from $\\textbf{h=Kh}_\\text{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm of LM method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnlm(ode,yhat,Q,k0,t,opts=[1e-3,1e-8,1e-8,1000]):\n",
    "    # Input arguments\n",
    "\n",
    "    # opts = [tau, tolg, tolk, max_iter]\n",
    "    #\n",
    "    # Outputs\n",
    "    # output = [k,Y,info]\n",
    "    # k : parameters\n",
    "    # Y : results with k\n",
    "    # info = [it,ter]\n",
    "    # it : Number of iterations\n",
    "    # ter : Termination criteria 1: gradient 2: change in h 3: maximum iteration\n",
    "\n",
    "    try:\n",
    "        stop = False\n",
    "        nu = 2\n",
    "        it = 0 \n",
    "        rho = 0\n",
    "        ter = 'm'\n",
    "        N = np.size(t)\n",
    "        if np.ndim(yhat) == 1:\n",
    "            scalar = True\n",
    "            n = 1\n",
    "            assert N == np.size(yhat), \"Dimension mismatch with yhat and t\"\n",
    "        else:\n",
    "            scalar = False\n",
    "            n = np.size(yhat,0)\n",
    "            assert N == np.size(yhat,1), \"Dimension mismatch with yhat and t\"\n",
    "        p = np.size(k0)\n",
    "        k = k0.copy()\n",
    "        Y,Jt,S,r,fail = checkSrJ(ode,yhat,k,t,n,p,N,Q,1e8,scalar)\n",
    "        assert not fail, \"Huge residuals\"\n",
    "        S0 = S\n",
    "        r0 = r.copy()\n",
    "        H,g = Hg(Jt,Q,r,p,N)\n",
    "        gn = LA.norm(g,np.inf)\n",
    "        stop = bool(gn < opts[1])\n",
    "        if stop:\n",
    "            ter = 'g'\n",
    "            print(\"First guess was correct!\")\n",
    "        mu = opts[0]*max(np.diag(H))\n",
    "        print('Iter | Obj func | step size | gradient |   mu   |   rho')\n",
    "        print(\"{0:5d}|{1:10.4e}|   Not cal |  Not cal |{2:8.1e}| Not cal\"\n",
    "              .format(it,S,mu))\n",
    "        while (not stop) and (it<=opts[3]):\n",
    "            fail = False\n",
    "            it += 1\n",
    "            h,mu = cholsolve(H,-g,mu,p)\n",
    "            hn = LA.norm(h,2)\n",
    "            kn = LA.norm(k,2)\n",
    "            if hn <= opts[2]*(kn+opts[2]):\n",
    "                stop = True\n",
    "                ter = 'h'\n",
    "            else:\n",
    "                k_new = k + h\n",
    "                Y,Jt,S,r,fail = checkSrJ(ode,yhat,k_new,t,n,p,N,Q,S0,scalar)\n",
    "                dL = h@(mu*h-g)/2\n",
    "                if dL>0 and not fail:\n",
    "                    df = dF(r0,r,Q,n,N)\n",
    "                    rho = df/dL\n",
    "                    k = k_new \n",
    "                    S0 = S\n",
    "                    r0 = r.copy()\n",
    "                    H, g = Hg(Jt,Q,r,p,N)\n",
    "                    gn = LA.norm(g,np.inf) \n",
    "                    if gn < opts[1]:\n",
    "                        stop = True\n",
    "                        ter = 'g'\n",
    "                    mu *= max(1/3,1-(2*rho-1)**3)\n",
    "                    nu = 2\n",
    "                else:\n",
    "                    mu *= nu\n",
    "                    nu *= 2\n",
    "            if rho == 0:\n",
    "                print(\"{0:5d}|{1:10.4e}|{2:11.3e}|{3:10.2e}|{4:8.1e}| Not cal\"\n",
    "                        .format(it,S,hn,gn,mu))\n",
    "            else:\n",
    "                print(\"{0:5d}|{1:10.4e}|{2:11.3e}|{3:10.2e}|{4:8.1e}|{5:8.1e}\"\n",
    "                        .format(it,S,hn,gn,mu,rho))\n",
    "        info = [it,ter]\n",
    "        output = [k,Y,info]\n",
    "        return output\n",
    "    except OverflowError as oerr:\n",
    "        print(oerr.args)\n",
    "        return\n",
    "    except AssertionError as aerr:\n",
    "        print(aerr.args)\n",
    "        return   \n",
    "\n",
    "    \n",
    "def checkSrJ(ode,yhat,k,t,n,p,N,Q,S0,scalar):\n",
    "    # initial condition J0 = 0\n",
    "    if scalar:\n",
    "        y0 = yhat[0]\n",
    "    else:\n",
    "        y0 = yhat[:,0]\n",
    "    fail = False    \n",
    "    r = np.zeros((n,N))    \n",
    "    Z = np.zeros((n*(p+1),N))\n",
    "    Z[0:n,0] = y0.copy()\n",
    "    S = 0\n",
    "    i = 0\n",
    "    Z, suc = integration(ode,Z[0],k,[t[0],t[-1]],n,p)\n",
    "    if not suc:\n",
    "        S = S0\n",
    "        fail = True\n",
    "    Y = Z[0:n]\n",
    "    S = 0\n",
    "    r = yhat-Y\n",
    "    if np.size(Q) == 1:\n",
    "        S = np.sum(r**2)\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            S += r[:,i]@Q@r[:,i]\n",
    "    if S > S0:\n",
    "        S = S0\n",
    "        fail = True\n",
    "    J = Z[n:]\n",
    "    Jt = np.hsplit(J,N)\n",
    "    for i in range(N):\n",
    "        Jt[i] = Jt[i].reshape(p,n).transpose()\n",
    "    return Y,Jt,S,r,fail\n",
    "\n",
    "def checkSrJ_step(ode,yhat,k,t,n,p,N,Q,S0,scalar):\n",
    "    # initial condition J0 = 0\n",
    "    if scalar:\n",
    "        y0 = yhat[0]\n",
    "    else:\n",
    "        y0 = yhat[:,0]\n",
    "        \n",
    "    r = np.zeros((n,N))    \n",
    "    Z = np.zeros((n*(p+1),N))\n",
    "    Z[0:n,0] = y0.copy()\n",
    "    S = 0\n",
    "    i = 0\n",
    "    fail = False\n",
    "    \n",
    "    while (not fail) and i<N-1:\n",
    "        Z[:,i+1],suc = integ_onestep(ode,Z[:,i],k,[t[i],t[i+1]],n,p)\n",
    "        if not suc:\n",
    "            S = S0\n",
    "            fail = True\n",
    "        else:\n",
    "            if scalar:            \n",
    "                r[:,i+1] = yhat[i+1]-Z[0,i+1]\n",
    "            else:\n",
    "                r[:,i+1] = yhat[:,i+1]-Z[0:n,i+1] \n",
    "            S += r[:,i+1]@Q@r[:,i+1]/2\n",
    "            if S>S0:\n",
    "                S = S0\n",
    "                fail = True\n",
    "        i += 1\n",
    "        \n",
    "    Y = Z[0:n]\n",
    "    J = Z[n:]\n",
    "    Jt = np.hsplit(J,N)\n",
    "    for i in range(N):\n",
    "        Jt[i] = Jt[i].reshape(p,n).transpose()\n",
    "    return Y,Jt,S,r,fail    \n",
    "\n",
    "def Hg(Jt,Q,r,p,N):\n",
    "    H = np.zeros((p,p))\n",
    "    g = np.zeros(p)\n",
    "    for i in range(N):\n",
    "        JQ = Jt[i].T@Q\n",
    "        H += JQ@Jt[i]\n",
    "        g -= JQ@r[:,i]\n",
    "    return H,g\n",
    "\n",
    "\n",
    "def dF(r0,r,Q,n,N):\n",
    "    dS = 0\n",
    "    if n == 1:\n",
    "        dS = (r0[0]-r[0])@(r0[0]+r[0])/2\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            dS += (r0[:,i]-r[:,i])@Q@(r0[:,i]+r[:,i])/2\n",
    "    return dS\n",
    "    \n",
    "def cholesky(A,p):\n",
    "    C = np.zeros((p,p))\n",
    "    j = 0\n",
    "    pd = True\n",
    "    while pd and j<p:\n",
    "        sum = 0\n",
    "        for k in range(j):\n",
    "            sum += C[j][k]**2\n",
    "        d = A[j][j]-sum\n",
    "        if d>0:\n",
    "            C[j][j] = np.sqrt(d)\n",
    "            for i in range(j,p):\n",
    "                sum = 0\n",
    "                for k in range(j):\n",
    "                    sum += C[i][k]*C[j][k]\n",
    "                C[i][j] = (A[i][j]-sum)/C[j][j]\n",
    "        else:\n",
    "            pd = False\n",
    "        j += 1\n",
    "    return C,pd\n",
    "\n",
    "def cholsolve(A,b,mu,p):\n",
    "    I = np.eye(p)\n",
    "    mA = np.amax(abs(A))\n",
    "    pd = False\n",
    "    while pd == False:\n",
    "        C,pd = cholesky(A+mu*I,p)\n",
    "        # check for near singularity\n",
    "        if pd == True:\n",
    "            pd = (1/LA.cond(C,1)>=1e-15)\n",
    "        if pd == False:\n",
    "            mu = max(10*mu,np.finfo(float).eps*mA)\n",
    "    # CC^Tx = b\n",
    "    z = np.zeros(p)\n",
    "    x = np.zeros(p)\n",
    "    # Forward C^Tx = z\n",
    "    for i in range(p):\n",
    "        sum = 0\n",
    "        for j in range(i):\n",
    "            sum += C[i][j]*z[j]\n",
    "        z[i] = (b[i]-sum)/C[i][i]\n",
    "    # Backward Cz = b\n",
    "    for i in reversed(range(p)):\n",
    "        sum = 0\n",
    "        for j in range(i,p):\n",
    "            sum += C[j][i]*x[j]\n",
    "        x[i] = (z[i]-sum)/C[i][i]\n",
    "    return x,mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "The stopping criteria $\\Vert\\mathbf{g}\\Vert\\leq\\varepsilon$ has disadvantage that it is not easy for the user to know what magnitude to choose for $\\varepsilon$ and does not have certain important invariance properties.\n",
    "\n",
    "For the affine transformation\n",
    "\\begin{equation}\n",
    "\\mathbf{y=Ax+a}\n",
    "\\end{equation}\n",
    "the gradient relation is\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_x=\\mathbf{A^\\top g}_y\n",
    "\\end{equation}\n",
    "and the Hessian relation is\n",
    "\\begin{equation}\n",
    "\\mathbf{G}_x=\\mathbf{A^\\top G}_y\\mathbf{A}\n",
    "\\end{equation}\n",
    "Newton-like method with the soft line search is invariant. However the steepest descent method is not invariant since $\\mathbf{I}$ does not transform correctly and so is modified Newton method since $\\mathbf{H}+\\mu\\mathbf{I}$ does not transform correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

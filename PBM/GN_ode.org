#+startup: latexpreview
* Virtual environment for Python
  #+begin_src bash :dir ./ :results drawer :exports none
    pwd
    virtualenv py3_venv
  #+end_src

  #+RESULTS:
  :results:
  /home/lsg/dev/Python/PBM
  Using base prefix '/usr'
  New python executable in /home/lsg/dev/Python/PBM/py3_venv/bin/python3
  Also creating executable in /home/lsg/dev/Python/PBM/py3_venv/bin/python
  Installing setuptools, pip, wheel...
  done.
  :end:
  #+begin_src elisp :results drawer :exports none
    (pyvenv-activate "home/lsg/dev/Python/PBM/py3_venv")
  #+end_src

  #+RESULTS:
  :results:
  nil
  :end:
  #+begin_src bash :results drawer :exports none
    pip install numpy matplotlib
  #+end_src

  #+RESULTS:
  :results:
  Collecting numpy
    Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting matplotlib
    Using cached https://files.pythonhosted.org/packages/32/6b/0368cfa5e1d1ae169ab7dc78addda3fd5e6262e48d7373a9114bac7caff7/matplotlib-2.2.4-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting cycler>=0.10 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
  Collecting backports.functools-lru-cache (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl
  Collecting subprocess32 (from matplotlib)
  Collecting kiwisolver>=1.0.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/3d/78/cb9248b2289ec31e301137cedbe4ca503a74ca87f88cdbfd2f8be52323bf/kiwisolver-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting pytz (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl
  Collecting six>=1.10 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl
  Collecting python-dateutil>=2.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl
  Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl
  Collecting setuptools (from kiwisolver>=1.0.1->matplotlib)
    Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl
  Installing collected packages: numpy, six, cycler, backports.functools-lru-cache, subprocess32, setuptools, kiwisolver, pytz, python-dateutil, pyparsing, matplotlib
  Successfully installed backports.functools-lru-cache-1.5 cycler-0.10.0 kiwisolver-1.1.0 matplotlib-2.2.4 numpy-1.16.4 pyparsing-2.4.0 python-dateutil-2.8.0 pytz-2019.1 setuptools-41.0.1 six-1.12.0 subprocess32-3.5.4
  :end:
* Gauss-Newton algorithm for ODE models
  #+begin_src python :session gnode :results output :tangle yes
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.integrate import solve_ivp, odeint
  #+end_src

The purpose is to produce
1. \(\varphi(z)\) that produces \(n\times(p+1)\)-dimension vector with input arguments of function, initial condition \(y_0\), and parameters \(k\).
2. Gauss-Newton parameter estimation procedure.

For the intial value problem
\begin{equation*}
\frac{d\mathbf{y}(t)}{dt}=\mathbf{f}(
\mathbf{y}(t),\mathbf{k});~~\mathbf{y}(t_0)=\mathbf{y}_0
\end{equation*}
the objective function with the weighting matrix $\mathbf{Q}_i$ is
\begin{equation*}
S(\mathbf{k})=\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]^\top\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]
\end{equation*}
** GN algorithm
1. Input initial guess for the parameters and tolerance
2. Repeat
  1) Integrate state and sensitivity equations to obtain $\mathbf{y}(t)$ and $\mathbf{J}(t)$. At each sampling period, $t_i$ $i=1,\cdots,N$ compute $\mathbf{y}(t_i,\mathbf{k}^{(j)})$, and $\mathbf{J}(t_i)$ to set up matrix $\mathbf{A}$ and vector $\mathbf{b}$.
  2) Solve the linear equation $\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}$
  3) Determine $\mu$ using the bisection rule and obtain $\mathbf{k}^{(j+1)}=\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)}$.
  4) Continue until the maximum number of iterations is reached or convergence is achieved

** Dimensions
n: dimension of given problem dim(y)
p: dimension of parameters dim(k)
N: dimension of measurements dim(t)
dim(J) = n*p
dim(Q) = n*n
dim(A) = p*p
dim(b) = p*1
dim(yhat) = n*N
dim(dfdy) = n*n
dim(dfdk) = n*p
So with N measurements,
Q = n*n*N; usually neglect effect of N
yhat = n*N
Jt_i : Jacobians for all \(t_1,t_2,\cdots,t_N\), n*p*N
*** Rule for variable dimensions in Python source code
 y : np.empty(n) 
 yhat : np.empty((N,n))
 t : np.empty(N)
 J : np.empty((n,p))
 k : np.empty(p)
 Q : np.eye(n) * Q is n*n*N. Usually, depence on N is not considered.
 A : np.empty((p,p))
 b : np.empty(p)

** Sensitivity matrix
The sensitivity or Jacobian matrix is
\begin{equation*}
\mathbf{J}(t_i)=\frac{\partial\mathbf{y}}{\partial\mathbf{k}}
\end{equation*}
In ODE models, the sensitivity matrix cannot be obtained by a simple differentiation. In ODE model, we can get differential equation for $\mathbf{J}$.
\begin{equation*}
\frac{d\mathbf{J}(t)}{dt}=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{J}(t)+\frac{\partial\mathbf{f}}{\partial\mathbf{k}};~~\mathbf{J}(t_0)=0
\end{equation*}
** A matrix and b vector
\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
\begin{equation*}
\mathbf{b}=\sum_{t=i}^N\mathbf{J}^\top(t_i)\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})]
\end{equation*}
for solving the linear equation
\begin{equation*}
\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}
\end{equation*}

** Construction of differential equations system
The sensitivity matrix is
\begin{equation*}
\mathbf{J}(t)=\frac{\partial\mathbf{y}}{\partial\mathbf{k}}=\left[\frac{\partial\mathbf{y}}{\partial k_1},\cdots,\frac{\partial\mathbf{y}}{\partial k_p}\right]=[\mathbf{g}_1,\cdots,\mathbf{g}_p]
\end{equation*}
where \(\mathbf{g}_i\) represents \(n\)-dimensional vector which is the sensitivity coefficients of the state variables with respect to parameter $k_i$. Each of $\mathbf{g}_i$ satisfies the differential equation for sensitivity matrix such that
\begin{equation*}
\frac{d\mathbf{g}_i(t)}{dt}=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_i+\frac{\partial\mathbf{f}}{\partial k_i};~~\mathbf{g}_p(t_0)=0;~~i=1,\cdots,p
\end{equation*}
We generate \(n\times(p+1)\)-dimensional differential equations system
\begin{equation*}
\frac{d\mathbf{z}}{dt}=\varphi(\mathbf{z})
\end{equation*}
$\mathbf{z}$ is \(n\times(p+1)\)-dimensional vector
\begin{equation*}
\mathbf{z}=\begin{bmatrix} \mathbf{x}(t)\\
                          \frac{\partial\mathbf{y}}{\partial k_1}\\
                          \vdots\\
                          \frac{\partial\mathbf{y}}{\partial k_p}
\end{bmatrix}
=\begin{bmatrix} \mathbf{y}(t)\\
                 \mathbf{g}_1(t)\\
                 \vdots\\
                 \mathbf{g}_p(t)
\end{bmatrix}
\end{equation*}
$\mathbf{\varphi}(\mathbf{z})$ is \(n\times(p+1)\)-dimensional vector function

\begin{equation*}
\mathbf{\varphi}(\mathbf{z})=\begin{bmatrix}
\mathbf{f}(\mathbf{y},\mathbf{k})\\
\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_1(t)+\frac{\partial\mathbf{f}}{\partial k_1}\\
\vdots\\
\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_p(t)+\frac{\partial\mathbf{f}}{\partial k_p}
\end{bmatrix}
\end{equation*}

To get the Jacobian for all $t_i$, \(\varphi(\mathbf{z}_i)\) should be solved for \(t_i,~~i=1,2,\cdots,N\).

#+name: dfdy
#+begin_src python :session gnode :exports code :tangle yes
  def dfdy_ode(func,y,k,n):
      h = 1e-8
      y = y.astype(np.float)
      if np.isscalar(y):
          dfdy = (func(y+h,k)-func(y-h,k))/(2*h)
          return dfdy
      else:
          dfdy = np.empty((n,n))
          for i in range(n):
              yr = y.copy()
              yl = y.copy()
              yr[i] += h
              yl[i] -= h
              dfdy[i] = (func(yr,k)-func(yl,k))/(2*h)
          return dfdy.transpose()
      return
#+end_src

#+name: dfdk
#+begin_src python :session gnode :exports code :tangle yes
  def dfdk_ode(func,y,k,n,p):
      h = 1e-8
      k = k.astype(np.float)
      if p == 1:
          dfdk = (func(y,k+h)-func(y,k-h))/(2*h)
          return dfdk
      else:
          dfdk = np.empty((p,n))
          for i in range(p):
              kr = k.copy()
              kl = k.copy()
              kr[i] += h
              kl[i] -= h
              dfdk[i] = (func(y,kr)-func(y,kl))/(2*h)
          return dfdk.transpose()
      return
#+end_src

#+name: z construction
#+begin_src python :exports code :tangle yes
  def phi_z(func,z,k,n,p):
      y = z[0:n]
      J = z[n:].reshape((p,n)).transpose()
      phiz = np.empty(n*(p+1))
      dfdy = dfdy_ode(func,y,k,n)
      dfdk = dfdk_ode(func,y,k,n,p)
      dJdt = dfdy@J+dfdk
      phiz[0:n] = func(y,k)
      phiz[n:] = dJdt.transpose().flatten()
      return phiz
#+end_src

The sensitivity matrix \(\textbf{J}\) is obtained by integration of \(\varphi(z)\). Integration of \(\varphi(z)\) returns \(n\times(p+1)\) vector
\begin{equation*}
\textbf{z}=\begin{bmatrix}
\textbf{y}\\
\textbf{g}_1\\
\textbf{g}_2\\
\vdots\\
\textbf{g}_p
\end{bmatrix}
\end{equation*}
where
\begin{equation*}
\textbf{g}_i=\begin{bmatrix}
\frac{\partial y_1}{\partial k_i}\\
\frac{\partial y_2}{\partial k_i}\\
\vdots\\
\frac{\partial y_n}{\partial k_i}
\end{bmatrix},~~~~i=1,\hdots,p
\end{equation*}
The sensitivity or Jacobian matrix \(\textbf{J}\) is
\begin{equation*}
\textbf{J}=\begin{bmatrix}
\textbf{g}_1,\textbf{g}_2,\cdots,\textbf{g}_p
\end{bmatrix}
\end{equation*}
To compute the \(\textbf{A}\) matrix
\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
the sensitivity matrix for all measurement time should be returned as \(n\times p\times N\) matrix.
The ODE solver for initial value problem returns \([n\times(p+1)]\times N\) matrix
\begin{matrix*}
Z=\begin{bmatrix}
y(t_1)&y(t_2)&\cdots&y(t_N)\\
g_1(t_1)&g_1(t_2)&\cdots&g_1(t_N)\\
\vdots&&\ddots&\vdots\\
g_p(t_1)&g_p(t_2)&\cdots&g_p(t_N)
\end{bmatrix}
\end{matrix*}
This matrix would be refomulated for
\begin{equation*}
\textbf{Y}=\begin{bmatrix}
y(t_1)&y(t_2)&\cdots&y(t_N)
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\textbf{J}t_i=\begin{bmatrix}
\textbf{g}_1(t_1)&\textbf{g}_2(t_1)&\cdots&\textbf{g}_p(t_1)
\end{bmatrix}
,\hdots,\begin{bmatrix}
\textbf{g}_1(t_N)&\textbf{g}_2(t_N)&\cdots&\textbf{g}_p(t_N)
\end{bmatrix}
\end{equation*}

#+name: ODE solving
#+begin_src python :session gnode :exports code :results none :tangle yes
  def state_jacob_int(func,y0,k,time):
      n = np.size(y0)
      p = np.size(k)
      N = np.size(time)
      # initial condition J0 = 0
      z0 = np.zeros(n*(p+1))
      z0[0:n] = y0
      def dzdt(t,z):
          return phi_z(func,z,k,n,p)
      solution = solve_ivp(dzdt,[time[0],time[-1]],z0,method='Radau',t_eval=time)
      if solution.success == False:
          raise OverflowError("Integration by state_jacob_int failed")
      Z = solution.y
      Y = Z[0:n]
      J = Z[n:]
      Jt_i = np.hsplit(J,N)
      for i in range(N):
          Jt_i[i] = Jt_i[i].reshape(p,n).transpose()
      return Y,Jt_i,solution.success

  def state_only_int(func,y0,k,time):
      def dydt(t,y):
          return func(y,k)
      solution = solve_ivp(dydt,[time[0],time[-1]],y0,method='Radau',t_eval=time)
      return solution.y,solution.success
#+end_src

** Construction of A and b and solve for \(\Delta k\).
With a particular point \(\mathbf{P}\) as the origin of the coordinate system with coordinates \(\mathbf{x}\), any function \(f\) can be apporximated by its Taylor series

\begin{align*}
f(\mathbf{x})=&f(\mathbf{P})+\sum_i\frac{\partial f}{\partial x_i}x_i+\frac{1}{2}\sum_{i,j}\frac{\partial^2 f}{\partial x_i\partial x_j}x_ix_j+\cdots\\
\approx&~~c-\mathbf{b}\cdot\mathbf{x}+\frac{1}{2}\mathbf{x}\cdot\mathbf{A}\cdot\mathbf{x}
\end{align*}
where, \(\mathbf{b}=-\nabla f|_{\mathbf{P}}\) and \(A_{ij}=\frac{\partial^2f}{\partial x_i\partial x_j}|_{\mathbf{P}}\) is the Hessian matrix. Diffentiation of this results in
\begin{equation*}
\nabla f=\mathbf{A}\cdot\mathbf{x}-\mathbf{b}
\end{equation*}
so that the function will be extreme where \(\mathbf{A}\cdot\mathbf{x}=\mathbf{b}\).
In nonlinear models, we want to minimize the \(\chi^2\) merit function
\begin{equation*}
\chi^2=\sum_{i=1}^N\left(\frac{\hat{y}_i-y(t_i\vert k_0,\cdots,k_p)}{\sigma_i}\right)^2
\end{equation*}
which will be approximated by Taylor series
\begin{equation*}
\chi^2(\mathbf{k})\approx\gamma-\mathbf{b}\cdot\mathbf{k}+\frac{1}{2}\mathbf{k}\cdot\mathbf{A}\cdot\mathbf{k}
\end{equation*}
With current estimation of parameter \(\mathbf{k}_\text{cur}\) we have
\begin{equation*}
\chi^2(\mathbf{k})=\chi^2(\mathbf{k}_\text{cur})+\nabla\chi^2(\mathbf{k}_\text{cur})\cdot(\mathbf{k}-\mathbf{k}_\text{cur})+\frac{1}{2}(\mathbf{k}-\mathbf{k}_\text{cur})\cdot\textbf{A}\cdot(\mathbf{k}-\mathbf{k}_\text{cur})
\end{equation*}
We want \(\nabla\chi^2(\mathbf{k})=0\) at \(\mathbf{k}_\text{min}\) such that
\begin{equation*}
\mathbf{k}_\text{min}=\textbf{k}_\text{cur}-\textbf{A}^{-1}\cdot\nabla\chi^2(\textbf{k}_\text{cur})
\end{equation*}
The gradient of \(\chi^2\) is (with \(y(t_i)=y(t_i\vert\mathbf{k})\))
\begin{equation*}
\frac{\partial\chi^2}{\partial k_j}=-2\sum_{i=1}^N\frac{(\hat{y}_i-y(t_i))}{\sigma_i^2}\frac{\partial y(t_i)}{\partial k_j}
\end{equation*}
and the Hessian is
\begin{equation*}
\frac{\partial^2\chi^2}{\partial k_j\partial k_l}=2\sum_{i=1}^N\frac{1}{\sigma_i^2}\left[\frac{\partial y(t_i)}{\partial k_j}\frac{\partial y(t_i)}{\partial k_l}-[\hat{y}_i-y(t_i)]\frac{\partial^2y(t_i)}{\partial k_j\partial k_l}\right]
\end{equation*}
Ignoring the second derivative of \(y\) and factoring out the common factor 2,
In \(n\)-dimensional \(y\),
\begin{equation*}
\nabla\chi^2=-2\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i)]^\top\mathbf{Q}_i\mathbf{J}_i
\end{equation*}
and
\begin{equation*}
\mathbf{H}(\chi^2)=2\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i
\end{equation*}
where \(\mathbf{J}_i=\frac{\partial\mathbf{y}(t_i)}{\partial\mathbf{k}}\).


\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
\begin{equation*}
\mathbf{b}=\sum_{t=i}^N\mathbf{J}^\top(t_i)\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})]
\end{equation*}
\begin{equation*}
\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}
\end{equation*}

#+name: Delta k
#+begin_src python :session gnode :exports code :tangle yes
  def delta_k(J,Q,yhat,Y,p,N):
      if np.shape(yhat) != np.shape(Y):
          raise ValueError('size mismatch of yhat and Y')
      Hessian = np.zeros((p,p))
      gradient = np.zeros(p)
      for i in range(N):
          JQ = J[i].T@Q
          Hessian += JQ@J[i]
          gradient += JQ@(yhat[:,i]-Y[:,i])
      # solve using singluar value decomposition
      def svdsolve(a,b):
          u,s,v = np.linalg.svd(a)
          c = u.T@b
          w = np.linalg.solve(np.diag(s),c)
          x = v.T@w
          return x
      del_k = svdsolve(Hessian,gradient)
      return del_k
#+end_src

** Bisection rule
1. Set the stepping parameter $\mu=1$.
2. Repeat
   1) Check $S(\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)})<S(\mathbf{k}^{(j)})$ and accept $\mathbf{k}^{(j+1)}=\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)}$ is it's satisfied.
   2) Halve $\mu$ if step 1) is not satisfied.
The objective function is
\begin{equation*}
\chi(\mathbf{k})=\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]^\top\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]
\end{equation*}


#+begin_src python :session gnode :results none :exports code :tangle yes
  def chi_squared(yhat,Y,Q,N):
      S = 0
      diff = yhat-Y
      if np.size(Q) == 1:
          S = np.sum(diff**2)
      else:
          for i in range(N):
              # S += np.dot(np.matmul(diff[:,i],Q),diff[:,i])
              S += diff[:,i]@Q@diff[:,i]
      return S

  def bisect(func,yhat,Q,k,time,iter_max):
      # check whether y is 1-dimensional
      try:
          if np.size(yhat) == np.size(yhat,0):
              y0 = yhat[0]
              N = np.size(yhat)
          else:
              y0 = yhat[:,0]
              N = np.size(yhat,1)
          p = np.size(k)
          Y,J,suc = state_jacob_int(func,y0,k,time)
          dk = delta_k(J,Q,yhat,Y,p,N)
          mu = 1.0
          S0 = chi_squared(yhat,Y,Q,N)
          for j in range(iter_max):
              k_next = k + mu * dk
              Y_next,fos = state_only_int(func,y0,k_next,time)
              if fos == False:
                  mu /= 2
              else:
                  S = chi_squared(yhat,Y_next,Q,N)
                  if S < S0:
                      break
                  mu /= 2
          return Y,Y_next,J,dk,mu
      except OverflowError:
          print("Problem with integration. Try with another parameter")
          return
#+end_src

** Optimal step-size policy
The Taylor series of the state vector with respect to \(\mu\) is
\begin{equation*}
\mathbf{y}(t,\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)})=\mathbf{y}(t,\mathbf{k}^{(j)})+\mu\mathbf{J}(t)\Delta\mathbf{k}^{(j+1)}+\frac{1}{2}\mu^2\mathbf{H}(\Delta\mathbf{k}^{(j+1)},\Delta\mathbf{k}^{(j+1)})
\end{equation*}
where \(\mathbf{H}\) is a bilinear operator. Alternatively in tensor form with Einstein convention,
\begin{equation*}
y_i(t,\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)})=y_i(t,\mathbf{k}^{(j)})+\mu J_{ij}(t)\Delta k_j+\frac{1}{2}\mu^2H_{ijk}(t)\Delta k_j\Delta k_k
\end{equation*}
where \(J_{ij}=\frac{\partial y_i}{\partial k_j}\) and \(H_{ijk}=\frac{\partial^2y_i}{\partial k_j\partial k_k}\).
Taylor series for \(\mu_a\) with known \(\mathbf{y}(t,\mathbf{k}^{(j+1)}),\mathbf{y}(t,\mathbf{k}^{(j)})\) is
\begin{equation*}
\mathbf{y}(t,\mathbf{k}^{(j)}+\mu_a\Delta\mathbf{k}^{(j+1)})=\mathbf{y}(t,\mathbf{k}^{(j)})+\mu_a\mathbf{J}\Delta\mathbf{k}^{(j+1)}+\mu_a^2\mathbf{r}(t)
\end{equation*}
so that
\begin{equation*}
\mathbf{r}(t)=\frac{1}{\mu_a^2}\left(\mathbf{y}(t,\mathbf{k}^{(j)}+\mu_a\Delta\mathbf{k}^{(j+1)})-\mathbf{y}(t,\mathbf{k}^{(j)})-\mu_a\mathbf{J}(t)\Delta\mathbf{k}^{(j+1)}\right)
\end{equation*}
With this remainder vector \(\mathbf{r}(t)\) as the approximation of second order term in Taylor series, the objective funcion
\begin{equation*}
\chi(\mathbf{k})=\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]^\top\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]
\end{equation*}
becomes
\begin{equation*}
\chi(\mu)=\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})-\mu\mathbf{J}(t_i)\Delta\mathbf{k}^{(j+1)}-\mu^2\mathbf{r}(t_i)]^\top\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})-\mu\mathbf{J}(t_i)\Delta\mathbf{k}^{(j+1)}-\mu^2\mathbf{r}(t_i)]
\end{equation*}
Applying the stationary criterion \(\partial\chi/\partial\mu=0\) yields
\begin{equation*}
\beta_0\mu^3+\beta_1\mu^2+\beta_2\mu+\beta_3=0
\end{equation*}
where
\begin{align*}
\beta_0=&2\sum_{i=1}^N\mathbf{r}(t_i)^\top\mathbf{Q}_i\mathbf{r}(t_i)\\
\beta_1=&3\sum_{i=1}^N\mathbf{r}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)\Delta\mathbf{k}^{(j+1)}\\
\beta_2=&\sum_{i=1}^N\left[\Delta\mathbf{k}^{(j+1)}^\top\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)\Delta\mathbf{k}^{(j+1)}-2\mathbf{r}(t_i)^\top\mathbf{Q}_i[\hat{\mathbf{y}}(t_i)-\mathbf{y}(t_i,\mathbf{k}^{(j)})]\right]\\
\beta_3=&-\sum_{i=1}^N\Delta\mathbf{k}^{(j+1)}\mathbf{J}(t_i)^\top\mathbf{Q}_i[\hat{\mathbf{y}}(t_i)-\mathbf{y}(t_i,\mathbf{k}^{(j)})]
\end{align*}

#+name: optimal step-size
#+begin_src python :session gnode :results none :exports code :tangle yes
  def optimal_step_size(Y,Y_next,yhat,J,dk,mu_a,Q,n,N):
      beta = np.zeros(4)
      for i in range(N):  
          # Jdk = np.matmul(J[i],dk)
          Jdk = J[i]@dk
          dy = yhat[:,i]-Y[:,i]
          r = (Y_next[:,i]-Y[:,i]-mu_a*Jdk)/mu_a**2
          # rQ = np.matmul(r,Q)
          rQ = r@Q
          # beta[0] += 2*np.matmul(rQ,r)
          beta[0] += 2*rQ@r
          # beta[1] += 3*np.matmul(rQ,Jdk)
          beta[1] += 3*rQ@Jdk
          # beta[2] += np.matmul(np.matmul(Jdk.transpose(),Q),Jdk)-2*np.matmul(rQ,dy)
          beta[2] += Jdk.transpose()@Q@Jdk-2*rQ@dy
          # beta[3] += -np.matmul(np.matmul(Jdk.transpose(),Q),dy)
          beta[3] += -Jdk.transpose()@Q@dy
      mu_opt = np.roots(beta)
      return mu_opt
#+end_src

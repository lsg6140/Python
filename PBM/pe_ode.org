#+startup: latexpreview
#+LaTeX_HEADER: \usepackage[ruled,vlined]{algorithm2e}
* Virtual environment for Python
  #+begin_src bash :dir ./ :results drawer :exports none
    pwd
    virtualenv py3_venv
  #+end_src

  #+RESULTS:
  :results:
  /home/lsg/dev/Python/PBM
  Using base prefix '/usr'
  New python executable in /home/lsg/dev/Python/PBM/py3_venv/bin/python3
  Also creating executable in /home/lsg/dev/Python/PBM/py3_venv/bin/python
  Installing setuptools, pip, wheel...
  done.
  :end:
  #+begin_src elisp :results drawer :exports none
    (pyvenv-activate "home/lsg/dev/Python/PBM/py3_venv")
  #+end_src

  #+RESULTS:
  :results:
  nil
  :end:
  #+begin_src bash :results drawer :exports none
    pip install numpy matplotlib
  #+end_src

  #+RESULTS:
  :results:
  Collecting numpy
    Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting matplotlib
    Using cached https://files.pythonhosted.org/packages/32/6b/0368cfa5e1d1ae169ab7dc78addda3fd5e6262e48d7373a9114bac7caff7/matplotlib-2.2.4-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting cycler>=0.10 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
  Collecting backports.functools-lru-cache (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl
  Collecting subprocess32 (from matplotlib)
  Collecting kiwisolver>=1.0.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/3d/78/cb9248b2289ec31e301137cedbe4ca503a74ca87f88cdbfd2f8be52323bf/kiwisolver-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl
  Collecting pytz (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl
  Collecting six>=1.10 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl
  Collecting python-dateutil>=2.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl
  Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)
    Using cached https://files.pythonhosted.org/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl
  Collecting setuptools (from kiwisolver>=1.0.1->matplotlib)
    Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl
  Installing collected packages: numpy, six, cycler, backports.functools-lru-cache, subprocess32, setuptools, kiwisolver, pytz, python-dateutil, pyparsing, matplotlib
  Successfully installed backports.functools-lru-cache-1.5 cycler-0.10.0 kiwisolver-1.1.0 matplotlib-2.2.4 numpy-1.16.4 pyparsing-2.4.0 python-dateutil-2.8.0 pytz-2019.1 setuptools-41.0.1 six-1.12.0 subprocess32-3.5.4
  :end:
* Parameter estimation for ODE models
  #+begin_src python :session peode :results output :tangle yes
    import numpy as np
    from numpy import linalg as LA
    import matplotlib.pyplot as plt
    from scipy.integrate import solve_ivp, odeint
  #+end_src

** ODE model and objective function
The purpose is to produce
1. \(\varphi(\mathbf{z})\) that produces \(n\times(p+1)\)-dimension vector with input arguments of function, initial condition \(\mathbf{y}_0\), and parameters \(\mathbf{k}\).
2. Levenberg-Marquardt parameter estimation procedure.

For the intial value problem
\begin{equation*}
\frac{d\mathbf{y}(t)}{dt}=\mathbf{f}(
\mathbf{y}(t),\mathbf{k});~~\mathbf{y}(t_0)=\mathbf{y}_0
\end{equation*}
the objective function with the weighting matrix $\mathbf{Q}_i$ is
\begin{equation*}
S(\mathbf{k})=\frac{1}{2}\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]^\top\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})]
\end{equation*}
where \(S(\mathbf{k}):\mathbb{R}^p\to\mathbb{R}\). We need to find \(\mathbf{k^*}\) with the smallest \(S\).
** Minimization of the objective function
Assume that \(S\) is differentiable and smooth that the Taylor expansion is valid,
\begin{equation*}
S(\mathbf{k}+\mathbf{h})=S(\mathbf{k})+\mathbf{h}^\top\mathbf{g}+\frac{1}{2}\mathbf{h}\top\mathbf{Hh}+\mathcal{O}(\Vert\mathbf{h}\Vert^3)
\end{equation*}
where \(\mathbf{g}\) is the gradient,
\begin{equation*}
\mathbf{g}\equiv\mathbf{S}'(\mathbf{k})=\begin{bmatrix}
\frac{\partial S}{\partial k_1}(\mathbf{k})\\
\vdots\\
\frac{\partial S}{\partial k_p}(\mathbf{k})\end{bmatrix}
\end{equation*}
and \(\mathbf{H}\) is the Hessian,
\begin{equation*}
\mathbf{H}\equiv\mathbf{S}''(\mathbf{k}),~~H_{ij}=\frac{\partial^2S}{\partial k_i\partial k_j}(\mathbf{k})
\end{equation*}
To \(\mathbf{k}^*\) be a local minimizer, \(\mathbf{g}(\mathbf{k}^*)=\mathbf{0}\) and \(\mathbf{H}(\mathbf{k}^*)\) is positive definite.
\(\mathbf{h}\) is a descent direction for \(S\) at \(\mathbf{k}\) if \(\mathbf{h}^\top\mathbf{S}'(\mathbf{k})<0\).
With \(S(\mathbf{k})=\frac{1}{2}\sum_{i=1}^N\mathbf{r}_i^\top\mathbf{Q}_i\mathbf{r}_i\) for the residual \(\mathbf{r}_i=\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})\) and \(\mathbf{J}_i(\mathbf{k})=\frac{\partial\mathbf{y}(t_i,\mathbf{k})}{\partial\mathbf{k}}\) (\(\frac{\partial\mathbf{r}}{\partial\mathbf{k}}=-\frac{\partial\mathbf{y}}{\partial\mathbf{k}}\)),
\begin{equation*}
\mathbf{g}=\mathbf{S'(k)}=-\sum_{i=1}^N\mathbf{J}_i\mathbf{Q}_i\mathbf{r}_i
\end{equation*}
*** The gradient method
The steepest descent direction is the opposite of the gradient,
\begin{equation*}
\mathbf{h}_{sd}=-\mathbf{S}'(\mathbf{k})
\end{equation*}
The final convergence is linear and often very slow. The method has quite good performance in the initial stage of the iterative process.
*** Newton's method
The condition that \(\mathbf{k}+\mathbf{h}\) is the stationary point,
\begin{equation*}
\mathbf{g}(\mathbf{k}+\mathbf{h})\simeq\mathbf{g}(\mathbf{k})+\mathbf{H}(\mathbf{k})\mathbf{h}=0
\end{equation*}
hence,
\begin{equation*}
\mathbf{H}\mathbf{h}_n=-\mathbf{g}
\end{equation*}
If \(\mathbf{H}\) is positive definite, then the method has quadratic convergence.
*** Trust region and damped methods
Model \(L\) of the \(F\),
\begin{equation*}
F(\mathbf{k+h})\simeq L(\mathbf{h})\equiv F(\mathbf{k})+\mathbf{h^\top c}+\frac{1}{2}\mathbf{h^\top Bh}
\end{equation*}
Typically, the model is a second order Taylor expansion of \(F\) around \(\mathbf{k}\).\\
In a trust region method assuming the model is sufficiently accurate inside a ball with radius \(\Delta\),
\begin{equation*}
\mathbf{h}_{tr}\equiv\operatorname*{argmin}_{\Vert\mathbf{h}\Vert\leq\Delta}L(\mathbf{h})
\end{equation*}
In a damped method,
\begin{equation*}
\mathbf{h}_{dm}\equiv\operatorname*{argmin}_\mathbf{h}\left(L(\mathbf{h})+\frac{1}{2}\mu\mathbf{h^\top h}\right)
\end{equation*}
The gain ratio is the ratio between the actual and predicted decrese in function value,
\begin{equation*}
\rho=\frac{F(\mathbf{k})-F(\mathbf{k+h})}{L(\mathbf{0})-L(\mathbf{h})}
\end{equation*}
With large \(\rho\), we can use larger step and small \(\rho\) we should use smaller step.

** Soft line search
With a descent direction \(\mathbf{h}\), the variation of the objective funtion \(S\) along the direction from the current point \(\mathbf{k}\) is
\begin{equation*}
\varphi(\alpha)=S(\mathbf{k}+\alpha\mathbf{h})
\end{equation*}
and
\begin{equation*}
\varphi'(\alpha)=\mathbf{h^\top S'(k}+\alpha\mathbf{h)}
\end{equation*}
Python source code for the calcuation of \(S(\mathbf{k}+\alpha\mathbf{h})\) and \(\mathbf{S}'(\mathbf{k}+\alpha\mathbf{h})\) for ODE is
#+name: S S'
#+begin_src python :session peode :exports code :tangle yes
  def Sg_ode(ode,yhat,Q,k,time):
      # check whether y is vector or matrix
      try:
          if np.size(yhat) == np.size(yhat,0):
              y0 = yhat[0]
              N = np.size(yhat)
          else:
              y0 = yhat[:,0]
              N = np.size(yhat,1)

          p = np.size(k)
          Y,J = state_jacob_int(ode,y0,k,time)
          S,r = objective_func(yhat,Y,Q,N)
          # calculation of S' = g
          g = np.zeros(p)
          for i in range(N):
              g -= J[i].T@Q@r[:,i]
          return S, g
      except OverflowError:
          print("Problem with integration. Try with another parameters")
          return
#+end_src

To ensure effective decrease of \(S\), the first condition is that \(\varphi\) is below the line \(y=\lambda(\alpha)\),
\begin{equation*}
\varphi(\alpha_s)\leq\lambda(\alpha_s)
\end{equation*}
where \(\lambda(\alpha)=\varphi(0)+\beta_1\varphi'(0)\alpha\) with \(0<\beta_1<0.5\). The parameter \(\beta_1\) is normally small such as 0.001.
To ensure that \(\alpha\) is not to small, the second condition is that the local slope is greater than the starting slope
\begin{equation*}
\varphi'(\alpha_s)\geq\beta_2\varphi'(0)
\end{equation*}
with \(\beta_1<\beta_2<1\).
For soft line search, an interval \([a,b]\) is chosen so that it contains acceptable points. Then the interval is successively reduced. We find a point \(\alpha\) in the interval and accept \(\alpha\) as \(\alpha_s\) if it satisfies conditions above. Otherwise, we reduce the interval to \([a,\alpha]\) or \([\alpha,b]\).
*** Algorithm of soft line search
1. If \(\mathbf{S'(k)=0}\implies\varphi'(0)=0\), we do nothing.
2. The initial choice \(a=0,b=1\) is used since \(\alpha=1\) is a good guess in the final steps of the iteration. The upper bound \(\alpha_{\text{max}}\) is supplied by the user.
3. Update the interval to \(a=b,b=\max(2b,\alpha_{\text{max}})\) while the first condition is satisfied but the second condition is not satisfied.
4. 1-3 set up the initial interval \([a,b]\) with the acceptalble \(\alpha\)-range.
5. Refine \(\alpha\) and \([a,b]\) as follows
6. The second order polynomial \(\psi(t)=\varphi(a)+\varphi'(a)(t-a)+c(t-a)^2\) satisfies \(\psi(a)=\varphi(a),~\psi'(a)=\varphi'(a)\), and \(\psi(b)=\varphi(b)\). If \(c>0\), then \(\psi\) has a minimum and we let \(\alpha\) be the minimizer. Otherwise \(\alpha\) is the midpoint of \([a,b]\).
7. \(\alpha\) should be in the middle 80% of the interval.
9. 

*** Soft line search Python code
#+name: slinesearch
#+begin_src python :session peode :exports code :tangle no
  def slinesearch(ode,yhat,Y,Q,k,time,S,g,h,opts=[1e-3,0.99,10,10]):
      # Input arguments
      # ode : input ODEs
      # yhat : measured values
      # Y : calculated value with the ODE integrator using k as parameters
      # k : estimated parameters
      # time : times at measurements
      # S : objective function calculated with k
      # g : gradient of objective function with k
      # h : step vector
      # opts : [beta_1, beta_2, max_eval, alpha_max]
      # beta_1 : default 1e-3
      # beta_2 : default 0.99
      # max_eval : default 10
      # alpha_max : default 10
      #
      # Outputs
      # output : [[k,S,g],info]
      # k : new parameters
      # S : objective function with new parameters
      # g : gradient of the objective function with new parameters
      # info[0] > 0 : alpha_s. successful call
      #         = 0 : h is not downhill
      #         = -1 : failed call
      # info[1] = slope ratio at the solution, phi'(alpha_s) / phi'(0)
      # info[2] = number of function evaluations

      info = np.array([0,1,0],dtype = float)
      output = np.array([k,S,g],info)

      # 1. S'(k) = 0
      dphi0 = np.dot(h,g)
      if dphi0 >= -10*np.eps*LA.norm(h)*LA.norm(g):
          info[0] = 0
          return output
      # 1.

      slope1 = opts[0] * dphi0
      slope2 = opts[1] * dphi0

      # 2.
      a = 0, b = min(1,opts[3])
      # 2.

      # 3.
      stop = False
      S0 = S
      while not stop:
          stop, phib, g = Sg_ode(ode,yhat,Q,k+b*h,time)
          info[2] += 1
          if stop:
              info[0] = 0
          else:
              dphib = np.dot(h,g)
              if phib < S0 + slope1*b:
                  info[0] = b
                  info[1] = dphib / dphi0
                  a = b, phia = phib, dphia = dphib
                  k += b*h
                  if dphib < min(slope2,0) and info[2] < opts[2]
                  and b < opts[3]:
                      if 2.5*b >= opts[3]:
                          b = opts[3]
                      else:
                          b *= 2
                  else:
                      stop = True
              else:
                  stop = True
      # 3.

      if int(stop) >= 0:
          stop = info[2] >= opts[2] or (b >= opts[3] and dphib < slope2)

          or (a > 0 and dphib >= slope2)

      # 6.
      while not stop
      :
          c = interpolate(a,b,phia,phib,dphia,n)
          stop, phic, g = Sg_ode(ode,yhat,Q,k+c*h,time)
          info[2] += 1
          if stop:
              info[0] = 0
          else:
              dphic = np.dot(g,h)
              if phic < S0 + slope1*c:
                  info[0] = c
                  info[1] = dphic/dphi0
                  k += c*h
                  S = phic
                  a = c, phia = phic, dphia = dphic
                  stop = dphic > slope2
              else:
                  b = c, phib = phic, dphib = dphic
          stop = stop or info[2] >= opts[2]
      # 6.

      output = np.array([k,S,g],info)
      return output
#+end_src

#+RESULTS: slinesearch

#+begin_src python :session peode :exports code :tangle yes
  def interpolate(a,b,phia,phib,dphia,n):
      d = b-a
      c = (phib-phia-d*dphia)/d**2
      if c>=5*n*np.eps*b:
          alpha = a-dphia/(2*c)
          d = 0.1*d
          alpha = min(max(alpha,a+d),b-d)
      else:
          alpha = (a+b)/2
      return alpha
#+end_src

** Gauss-Newton method
With \(\mathbf{r}_i=\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})\), linear model of \(\mathbf{r}_i\) with Taylor expansion, \(\mathbf{l}_i\) is
\(\mathbf{r}_i(\mathbf{k+h})\simeq\mathbf{l}_i(\mathbf{h})=\mathbf{r}_i(\mathbf{k})-\mathbf{J}_i\mathbf{(k)h}\). Inserting \(\mathbf{l}_i\) to the objective function,
\begin{align*}
S(\mathbf{k+h})&\simeq L(\mathbf{h})\equiv\frac{1}{2}\sum_{i=1}^N\mathbf{l}_i\mathbf{(h)}^\top\mathbf{Q}_i \mathbf{l}_i(\mathbf{h})\\
                                   &=\frac{1}{2}\sum_{i=1}^N\left[\mathbf{r}_i\mathbf{(k)}^\top\mathbf{Q}_i\mathbf{r}_i\mathbf{(k)}-\mathbf{r}_i^\top\mathbf{Q}_i\mathbf{J}_i\mathbf{(k)h}-\mathbf{h}^\top\mathbf{J}_i\mathbf{(k)}^\top\mathbf{Q}_i\mathbf{r}_i(\mathbf{k})+\mathbf{h}^\top\mathbf{J}_i\mathbf{(k)}^\top\mathbf{Q}_i\mathbf{J}_i\mathbf{(k)}\mathbf{h}\right]
\end{align*}
Since \(L\) is a scalar function of \(\mathbf{h}\), 
\begin{equation*}
L(\mathbf{h})=S(\mathbf{k})-\sum_{i=1}^N\mathbf{h^\top J}_i^\top\mathbf{Q}_i \mathbf{r}_i+\frac{1}{2}\sum_{i=1}^N\mathbf{h^\top J}_i^\top\mathbf{Q}_i\mathbf{J}_i\mathbf{h}
\end{equation*}
The gradient and Hessian of \(L\) are
\begin{equation*}
\mathbf{L'(h)}=\sum_{i=1}^N\left[-\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{r}_i+\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i\mathbf{h}\right]~~~~~~~\mathbf{L''(h)}=\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i
\end{equation*}
The Hessian is independent of \(\mathbf{h}\), symmetric and positive definite if \(\mathbf{J}\) has full rank. Hence \(L\) is minimum when \(\mathbf{L'(h)}\) is zero vector. Hence,
\begin{equation*}
\left[\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i\right]\mathbf{h}_{gn}=\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{r}_i=\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\left(\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k})\right)
\end{equation*}
GN method is not quadratic convergent if \(\mathbf{r}_i\) is not zero around the solution.

** Levenberg-Marquardt method 
In Levenberg-Marquardt method, GN method is used with a damping term,
\begin{equation*}
\left[\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i+\mu\mathbf{I}\right]\mathbf{h}_\text{lm}=\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{r}_i
\end{equation*}
For large \(\mu\), \(\mathbf{h}_\text{lm}\simeq-\frac{1}{\mu}\mathbf{L'(0)}\) is a short step in the steepest descent direction. This is good if the estimation is far from the solution. If \(\mu\) is very small, LM method is nearly GN method which is almost quadratic convergent if \(S(\mathbf{k})\) is close to zero.
*** Initial \(\mu\)
The initial value of \(\mu\) is maximum diagonal element of \(\mathbf{H}_0=\sum\mathbf{J}_i(\mathbf{k}_0)^\top\mathbf{Q}_i\mathbf{J}_i(\mathbf{k}_0)\),
\begin{equation*}
\mu_0=\tau\cdot\max_ih_{ii}^0
\end{equation*}
where \(\tau\) is usually by a rule of thumb \(10^{-3}\) or \(10^{-6}\) if \(\mathbf{k}_0\) is believed to be a good approximation of the solution.
*** Gain ratio
The updating of \(\mu\) is controlled by the gain ratio
\begin{equation*}
\rho=\frac{S(\mathbf{k})-S(\mathbf{k+h}_\text{lm})}{L(\mathbf{0})-L(\mathbf{h}_\text{lm})}
\end{equation*}
The denominator is the gain predicted by the linear model,
\begin{align*}
L(\mathbf{0})-L(\mathbf{h}_\text{lm})=&\mathbf{h}_\text{lm}^\top\sum_{i=1}^N\mathbf{J}_i\mathbf{Q}_i\mathbf{r}_i-\frac{1}{2}\mathbf{h}^\top_\text{lm}\left[\sum_{i=1}^N\mathbf{J}_i\top\mathbf{Q}_i\mathbf{J}_i\right]\mathbf{h}_\text{lm}\\
                   =&\frac{1}{2}\mathbf{h}_\text{lm}^\top\left(2\sum_{i=1}^N\mathbf{J}_i\mathbf{Q}_i\mathbf{r}_i-\left[\sum_{i=1}^N\mathbf{J}^\top_i\mathbf{Q}_i\mathbf{J}_i+\mu\mathbf{I}\right]\mathbf{h}_\text{lm}+\mu\mathbf{I}\mathbf{h}_\text{lm}\right)\\
                   =&\frac{1}{2}\mathbf{h}_\text{lm}^\top\left(\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{r}_i+\mu\mathbf{h}_\text{lm}\right)\\
                   =&\frac{1}{2}\mathbf{h}_\text{lm}^\top\left(-\mathbf{g}+\mu\mathbf{h}_\text{lm}\right)
\end{align*}
A large value of \(\rho\) indicates that \(L(\mathbf{h}_\text{lm})\) is a good approximation of \(S(\mathbf{k+h}_\text{lm})\) so \(\mu\) can be decreased so that LM is closer to GN method.
*** Update of \(\mu\)
In  a damped method, small \(\rho\) indicates that the linear model is not a good approximation so the damping factor should increase and vice versa for the large \(\rho\). From Nielsen (1999), if \(\rho>0\), we update \(\mu\) to \(\mu\times\max\{\frac{1}{3},1-(2\rho-1)^3\}\) and \(\nu=2\). Otherwise, we update \(\mu\) to \(\mu\times\nu\) and \(\nu\) to \(2\nu\). 

*** Stopping criteria
1. At the global minimizer, \(\mathbf{F'(k)}=\mathbf{g(k)}=\mathbf{0}\).
\begin{equation*}
\Vert\mathbf{g}\Vert\leq\varepsilon_1
\end{equation*}
2. The change in \(\mathbf{k}\) is too small
\begin{equation*}
\Vert\mathbf{k}_{n+1}-\mathbf{k}_n\Vert\leq\varepsilon_2(\Vert\mathbf{k}_n\Vert+\varepsilon_2)
\end{equation*}

** Quasi-Newton method
Quasi-Newton method is a superlinear convergent method even if \(F(\mathbf{k^*})\) is not close to zero which gives GN a linear convergence not a quadratic convergence. So when \(F(\mathbf{k}^*)\) is significantly nonzero, QN has better performance. QN method is based on the approximation of Hessian by \(\mathbf{B}\) updated by the BFGS st


** GN algorithm
1. Input initial guess for the parameters and tolerance
2. Repeat
  1) Integrate state and sensitivity equations to obtain $\mathbf{y}(t)$ and $\mathbf{J}(t)$. At each sampling period, $t_i$ $i=1,\cdots,N$ compute $\mathbf{y}(t_i,\mathbf{k}^{(j)})$, and $\mathbf{J}(t_i)$ to set up matrix $\mathbf{A}$ and vector $\mathbf{b}$.
  2) Solve the linear equation $\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}$
  3) Determine $\mu$ using the bisection rule and obtain $\mathbf{k}^{(j+1)}=\mathbf{k}^{(j)}+\mu\Delta\mathbf{k}^{(j+1)}$.
  4) Continue until the maximum number of iterations is reached or convergence is achieved

** Dimensions
n: dimension of given problem dim(y)
p: dimension of parameters dim(k)
N: dimension of measurements dim(t)
dim(J) = n*p
dim(Q) = n*n
dim(A) = p*p
dim(b) = p*1
dim(yhat) = n*N
dim(dfdy) = n*n
dim(dfdk) = n*p
So with N measurements,
Q = n*n*N; usually neglect effect of N
yhat = n*N
Jt_i : Jacobians for all \(t_1,t_2,\cdots,t_N\), n*p*N
*** Rule for variable dimensions in Python source code
 y : np.empty(n) 
 yhat : np.empty((n,N))
 t : np.empty(N)
 J : np.empty((n,p))
 k : np.empty(p)
 Q : np.eye(n) * Q is n*n*N. Usually, dependence on N is not considered.
 A : np.empty((p,p))
 b : np.empty(p)

** TODO Sensitivity matrix
The sensitivity or Jacobian matrix is
\begin{equation*}
\mathbf{J}(t_i)=\frac{\partial\mathbf{y}}{\partial\mathbf{k}}
\end{equation*}
In ODE models, the sensitivity matrix cannot be obtained by a simple differentiation. In ODE model, we can get differential equation for $\mathbf{J}$.
Differentiate both side of \( \frac{d\mathbf{y}}{dt}=\mathbf{f}(\mathbf{y},t,\mathbf{k})\) and apply the chain rule,
\begin{align*}
\frac{\partial}{\partial\mathbf{ k}}\left(\frac{d\mathbf{y}}{dt}\right)&=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\frac{d\mathbf{y}}{d\mathbf{k}}+\frac{\partial\mathbf{f}}{\partial t}\frac{dt}{d\mathbf{k}}+\frac{\partial\mathbf{f}}{\partial\mathbf{k}}\frac{d\mathbf{k}}{d\mathbf{k}}\\
&=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\frac{d\mathbf{y}}{d\mathbf{k}}+\frac{\partial\mathbf{f}}{\partial\mathbf{k}}
\end{align*}
since \(dt/d\mathbf{k}\) is zero. Hence,
\begin{equation*}
\frac{d\mathbf{J}(t)}{dt}=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{J}(t)+\frac{\partial\mathbf{f}}{\partial\mathbf{k}};~~\mathbf{J}(t_0)=0
\end{equation*}
** A matrix and b vector
\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
\begin{equation*}
\mathbf{b}=\sum_{t=i}^N\mathbf{J}^\top(t_i)\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})]
\end{equation*}
for solving the linear equation
\begin{equation*}
\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}
\end{equation*}

** Construction of differential equations system
The sensitivity matrix is
\begin{equation*}
\mathbf{J}(t)=\frac{\partial\mathbf{y}}{\partial\mathbf{k}}=\left[\frac{\partial\mathbf{y}}{\partial k_1},\cdots,\frac{\partial\mathbf{y}}{\partial k_p}\right]=[\mathbf{g}_1,\cdots,\mathbf{g}_p]
\end{equation*}
where \(\mathbf{g}_i\) represents \(n\)-dimensional vector which is the sensitivity coefficients of the state variables with respect to parameter $k_i$. Each of $\mathbf{g}_i$ satisfies the differential equation for sensitivity matrix such that
\begin{equation*}
\frac{d\mathbf{g}_i(t)}{dt}=\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_i+\frac{\partial\mathbf{f}}{\partial k_i};~~\mathbf{g}_p(t_0)=0;~~i=1,\cdots,p
\end{equation*}
We generate \(n\times(p+1)\)-dimensional differential equations system
\begin{equation*}
\frac{d\mathbf{z}}{dt}=\varphi(\mathbf{z})
\end{equation*}
$\mathbf{z}$ is \(n\times(p+1)\)-dimensional vector
\begin{equation*}
\mathbf{z}=\begin{bmatrix} \mathbf{x}(t)\\
                          \frac{\partial\mathbf{y}}{\partial k_1}\\
                          \vdots\\
                          \frac{\partial\mathbf{y}}{\partial k_p}
\end{bmatrix}
=\begin{bmatrix} \mathbf{y}(t)\\
                 \mathbf{g}_1(t)\\
                 \vdots\\
                 \mathbf{g}_p(t)
\end{bmatrix}
\end{equation*}
$\mathbf{\varphi}(\mathbf{z})$ is \(n\times(p+1)\)-dimensional vector function

\begin{equation*}
\mathbf{\varphi}(\mathbf{z})=\begin{bmatrix}
\mathbf{f}(\mathbf{y},\mathbf{k})\\
\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_1(t)+\frac{\partial\mathbf{f}}{\partial k_1}\\
\vdots\\
\frac{\partial\mathbf{f}}{\partial\mathbf{y}}\mathbf{g}_p(t)+\frac{\partial\mathbf{f}}{\partial k_p}
\end{bmatrix}
\end{equation*}

To get the Jacobian for all $t_i$, \(\varphi(\mathbf{z}_i)\) should be solved for \(t_i,~~i=1,2,\cdots,N\).

#+name: dfdy
#+begin_src python :session peode :exports code :tangle yes
  def dfdy_ode(ode,y,k,n):
      h = 1e-8
      y = y.astype(np.float)
      if np.isscalar(y):
          dfdy = (ode(y+h,k)-ode(y-h,k))/(2*h)
          return dfdy
      else:
          dfdy = np.empty((n,n))
          for i in range(n):
              yr = y.copy()
              yl = y.copy()
              yr[i] += h
              yl[i] -= h
              dfdy[i] = (ode(yr,k)-ode(yl,k))/(2*h)
          return dfdy.transpose()
      return
#+end_src

#+name: dfdk
#+begin_src python :session peode :exports code :tangle yes
  def dfdk_ode(ode,y,k,n,p):
      h = 1e-8
      k = k.astype(np.float)
      if p == 1:
          dfdk = (ode(y,k+h)-ode(y,k-h))/(2*h)
          return dfdk
      else:
          dfdk = np.empty((p,n))
          for i in range(p):
              kr = k.copy()
              kl = k.copy()
              kr[i] += h
              kl[i] -= h
              dfdk[i] = (ode(y,kr)-ode(y,kl))/(2*h)
          return dfdk.transpose()
      return
#+end_src

#+name: z construction
#+begin_src python :exports code :tangle yes
  def phi_z(ode,z,k,n,p):
      y = z[0:n]
      J = z[n:].reshape((p,n)).transpose()
      phiz = np.empty(n*(p+1))
      dfdy = dfdy_ode(ode,y,k,n)
      dfdk = dfdk_ode(ode,y,k,n,p)
      dJdt = dfdy@J+dfdk
      phiz[0:n] = ode(y,k)
      phiz[n:] = dJdt.transpose().flatten()
      return phiz
#+end_src

The sensitivity matrix \(\textbf{J}\) is obtained by integration of \(\varphi(z)\). Integration of \(\varphi(z)\) returns \(n\times(p+1)\) vector
\begin{equation*}
\textbf{z}=\begin{bmatrix}
\textbf{y}\\
\textbf{g}_1\\
\textbf{g}_2\\
\vdots\\
\textbf{g}_p
\end{bmatrix}
\end{equation*}
where
\begin{equation*}
\textbf{g}_i=\begin{bmatrix}
\frac{\partial y_1}{\partial k_i}\\
\frac{\partial y_2}{\partial k_i}\\
\vdots\\
\frac{\partial y_n}{\partial k_i}
\end{bmatrix},~~~~i=1,\hdots,p
\end{equation*}
The sensitivity or Jacobian matrix \(\textbf{J}\) is
\begin{equation*}
\textbf{J}=\begin{bmatrix}
\textbf{g}_1,\textbf{g}_2,\cdots,\textbf{g}_p
\end{bmatrix}
\end{equation*}
To compute the \(\textbf{A}\) matrix
\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
the sensitivity matrix for all measurement time should be returned as \(n\times p\times N\) matrix.
The ODE solver for initial value problem returns \([n\times(p+1)]\times N\) matrix
\begin{equation*}
Z=\begin{bmatrix}
y(t_1)&y(t_2)&\cdots&y(t_N)\\
g_1(t_1)&g_1(t_2)&\cdots&g_1(t_N)\\
\vdots&&\ddots&\vdots\\
g_p(t_1)&g_p(t_2)&\cdots&g_p(t_N)
\end{bmatrix}
\end{equation*}
This matrix would be refomulated for
\begin{equation*}
\textbf{Y}=\begin{bmatrix}
y(t_1)&y(t_2)&\cdots&y(t_N)
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\textbf{J}t_i=\begin{bmatrix}
\textbf{g}_1(t_1)&\textbf{g}_2(t_1)&\cdots&\textbf{g}_p(t_1)
\end{bmatrix}
,\hdots,\begin{bmatrix}
\textbf{g}_1(t_N)&\textbf{g}_2(t_N)&\cdots&\textbf{g}_p(t_N)
\end{bmatrix}
\end{equation*}

#+name: ODE solving
#+begin_src python :session peode :exports code :results none :tangle yes
  def state_jacob_int(ode,y0,k,time):
      n = np.size(y0)
      p = np.size(k)
      N = np.size(time)
      # initial condition J0 = 0
      z0 = np.zeros(n*(p+1))
      z0[0:n] = y0
      def dzdt(t,z):
          return phi_z(ode,z,k,n,p)
      solution = solve_ivp(dzdt,[time[0],time[-1]],z0,method='Radau',t_eval=time)
      if solution.success == False:
          raise OverflowError("Integration by state_jacob_int failed")
      Z = solution.y
      Y = Z[0:n]
      J = Z[n:]
      Jt_i = np.hsplit(J,N)
      for i in range(N):
          Jt_i[i] = Jt_i[i].reshape(p,n).transpose()
      return Y,Jt_i

  def state_only_int(ode,y0,k,time):
      def dydt(t,y):
          return ode(y,k)
      solution = solve_ivp(dydt,[time[0],time[-1]],y0,method='Radau',t_eval=time)
      return solution.y,solution.success
#+end_src

** Construction of A and b and solve for \(\Delta k\).
With a particular point \(\mathbf{P}\) as the origin of the coordinate system with coordinates \(\mathbf{x}\), any function \(f\) can be apporximated by its Taylor series

\begin{align*}
f(\mathbf{x})=&f(\mathbf{P})+\sum_i\frac{\partial f}{\partial x_i}x_i+\frac{1}{2}\sum_{i,j}\frac{\partial^2 f}{\partial x_i\partial x_j}x_ix_j+\cdots\\
\approx&~~c-\mathbf{b}\cdot\mathbf{x}+\frac{1}{2}\mathbf{x}\cdot\mathbf{A}\cdot\mathbf{x}
\end{align*}
where, \(\mathbf{b}=-\nabla f|_{\mathbf{P}}\) and \(A_{ij}=\frac{\partial^2f}{\partial x_i\partial x_j}|_{\mathbf{P}}\) is the Hessian matrix. Diffentiation of this results in
\begin{equation*}
\nabla f=\mathbf{A}\cdot\mathbf{x}-\mathbf{b}
\end{equation*}
so that the function will be extreme where \(\mathbf{A}\cdot\mathbf{x}=\mathbf{b}\).
In nonlinear models, we want to minimize the \(\chi^2\) merit function
\begin{equation*}
\chi^2=\sum_{i=1}^N\left(\frac{\hat{y}_i-y(t_i\vert k_0,\cdots,k_p)}{\sigma_i}\right)^2
\end{equation*}
which will be approximated by Taylor series
\begin{equation*}
\chi^2(\mathbf{k})\approx\gamma-\mathbf{b}\cdot\mathbf{k}+\frac{1}{2}\mathbf{k}\cdot\mathbf{A}\cdot\mathbf{k}
\end{equation*}
With current estimation of parameter \(\mathbf{k}_\text{cur}\) we have
\begin{equation*}
\chi^2(\mathbf{k})=\chi^2(\mathbf{k}_\text{cur})+\nabla\chi^2(\mathbf{k}_\text{cur})\cdot(\mathbf{k}-\mathbf{k}_\text{cur})+\frac{1}{2}(\mathbf{k}-\mathbf{k}_\text{cur})\cdot\textbf{A}\cdot(\mathbf{k}-\mathbf{k}_\text{cur})
\end{equation*}
We want \(\nabla\chi^2(\mathbf{k})=0\) at \(\mathbf{k}_\text{min}\) such that
\begin{equation*}
\mathbf{k}_\text{min}=\textbf{k}_\text{cur}-\textbf{A}^{-1}\cdot\nabla\chi^2(\textbf{k}_\text{cur})
\end{equation*}
The gradient of \(\chi^2\) is (with \(y(t_i)=y(t_i\vert\mathbf{k})\))
\begin{equation*}
\frac{\partial\chi^2}{\partial k_j}=-2\sum_{i=1}^N\frac{(\hat{y}_i-y(t_i))}{\sigma_i^2}\frac{\partial y(t_i)}{\partial k_j}
\end{equation*}
and the Hessian is
\begin{equation*}
\frac{\partial^2\chi^2}{\partial k_j\partial k_l}=2\sum_{i=1}^N\frac{1}{\sigma_i^2}\left[\frac{\partial y(t_i)}{\partial k_j}\frac{\partial y(t_i)}{\partial k_l}-[\hat{y}_i-y(t_i)]\frac{\partial^2y(t_i)}{\partial k_j\partial k_l}\right]
\end{equation*}
Ignoring the second derivative of \(y\) and factoring out the common factor 2,
In \(n\)-dimensional \(y\),
\begin{equation*}
\nabla\chi^2=-2\sum_{i=1}^N[\hat{\mathbf{y}}_i-\mathbf{y}(t_i)]^\top\mathbf{Q}_i\mathbf{J}_i
\end{equation*}
and
\begin{equation*}
\mathbf{H}(\chi^2)=2\sum_{i=1}^N\mathbf{J}_i^\top\mathbf{Q}_i\mathbf{J}_i
\end{equation*}
where \(\mathbf{J}_i=\frac{\partial\mathbf{y}(t_i)}{\partial\mathbf{k}}\).


\begin{equation*}
\mathbf{A}=\sum_{i=1}^N\mathbf{J}(t_i)^\top\mathbf{Q}_i\mathbf{J}(t_i)
\end{equation*}
\begin{equation*}
\mathbf{b}=\sum_{t=i}^N\mathbf{J}^\top(t_i)\mathbf{Q}_i[\hat{\mathbf{y}}_i-\mathbf{y}(t_i,\mathbf{k}^{(j)})]
\end{equation*}
\begin{equation*}
\mathbf{A}\Delta\mathbf{k}^{(j+1)}=\mathbf{b}
\end{equation*}

#+name: Delta k
#+begin_src python :session peode :exports code :tangle yes
  def delta_k(J,Q,yhat,Y,p,N):
      if np.shape(yhat) != np.shape(Y):
          raise ValueError('size mismatch of yhat and Y')
      Hessian = np.zeros((p,p))
      gradient = np.zeros(p)
      for i in range(N):
          JQ = J[i].T@Q
          Hessian += JQ@J[i]
          gradient += JQ@(yhat[:,i]-Y[:,i])
      # solve using singluar value decomposition
      def svdsolve(a,b):
          u,s,v = LA.svd(a)
          c = u.T@b
          w = LA.solve(np.diag(s),c)
          x = v.T@w
          return x
      del_k = svdsolve(Hessian,gradient)
      return del_k
#+end_src


\begin{algorithm}[H]
 \SetAlgoLined
 \DontPrintSemicolon
 \Begin{
 $\nu = 2$\;
 Calculate $\textbf{Y}$ and $\textbf{J}$ with an ODE integrator using $\textbf{k}_0$\;
 Calculate $S$ and $\mathbf{r}$ using $\textbf{Y}$\; 
 function evaluation++\;
 S0 = S\;
 Calculate $\textbf{H}$ and $\textbf{g}$ using $\textbf{J}$ and $\textbf{r}$\;
 $\mu = \tau\cdot\max(\text{diag}(\mathbf{H}))$\;
 stop = bool({$\Vert\mathbf{g}\Vert_{\inf} \leq\varepsilon_1$})\;
 \While{\textbf{not} stop \textbf{and} $\text{iteration}\leq\text{iteration}_{max}$ }{
   iteration++\;
   Solve $(\mathbf{H}+\mu\mathbf{I})\mathbf{h}_\text{lm}=-\mathbf{g}$\;
   \eIf{$(\Vert\mathbf{h}_\text{lm}\Vert_{\inf}\leq\varepsilon_2(\Vert\mathbf{k}\Vert_{\inf}+\varepsilon_2))$}{
     stop = true\;
   }{
     $\mathbf{k}_\text{new}=\mathbf{k}+\mathbf{h}_\text{lm}$\;
     Calculate $\textbf{Y}$ and $\textbf{J}$ using $\textbf{k}_\text{new}$\;
     Calculate $S$ and $\textbf{r}$ using $\mathbf{Y}$\;
     function evaluation++\;
     Calculate $\rho$ using $S0$, $S$, $\mathbf{g}$, and $\mathbf{h}_\text{lm}$\;
     \eIf{$\rho>0$}{
       $\textbf{k}=\textbf{k}_\text{new}$\;
       S0 = S\;
       Calculate $\textbf{H}$ and $\textbf{g}$ using $\mathbf{J}$ and $\mathbf{r}$\;
       stop = bool($\Vert\textbf{g}\Vert_{\inf}\leq\varepsilon_1$)\;
       $\mu=\mu\cdot\max\left\{ \frac{1}{3},1-(2\rho-1)^3 \right\}$\;
       $\nu=2$\;
     }{
       $\mu=\mu\cdot\nu$\;
       $\nu=2\nu$\;
     }
   }
 }
 }
 \caption{Levenberg-Marquardt method for ODEs}
\end{algorithm}

#+name: Levenberg-Marquardt method
#+begin_src python :session peode :exports code :tangle yes
  def lm(ode,yhat,Q,k0,time,opts=[1e-3,1e-4,1e-8,100,1000]):
      # Input arguments

      # opts = [tau, tolg, tolk, max_eval, max_iter]
      #
      # Outputs
      # output = [k,Y,info]
      # k : parameters
      # Y : results with k
      # info = [eval, iter]

      try:
          stop = False
          nu = 2
          eval = 0
          iter = 0
          if np.size(yhat) == np.size(yhat,0):
              y0 = yhat[0]
              N = np.size(yhat)
          else:
              y0 = yhat[:,0]
              N = np.size(yhat,1)
          print('y0 is')
          print(y0)
          p = np.size(k0)
          I = np.eye(p)
          k = k0
          Y, J = state_jacob_int(ode,y0,k,time)
          S, r = objective_func(yhat,Y,Q,N)
          eval += 1
          S0 = S
          H,g = Hg(J,Q,r,p,N)
          stop = bool(LA.norm(g,np.inf) < opts[1])
          mu = opts[0]*max(np.diag(H))
          while not stop and eval <= opts[3] and iter<=opts[4]:
              iter += 1
              h = svdsolve(H+mu*I,-g)
              if LA.norm(h,np.inf) <= opts[2]*(LA.norm(k,np.inf)+opts[2]):
                  stop = True
              else:
                  k_new = k + h
                  Y, J = state_jacob_int(ode,y0,k_new,time)
                  eval += 1
                  S, r = objective_func(yhat,Y,Q,N)
                  rho = (S0 - S) / (h.T@(-g+mu*h)/2)
                  if rho >0:
                      k = k_new
                      S0 = S
                      H, g = Hg(J,Q,r,p,N)
                      stop = bool(LA.norm(g,np.inf) < opts[1])
                      mu *= max(1/3,1-(2*rho-1)**3)
                      nu = 2
                  else:
                      mu *= nu
                      nu *= 2
          info = [eval,iter]
          output = [k,Y,info]
          return output
      except OverflowError:
          print("Problem with integration. Try with another parameter")
          return

  def objective_func(yhat,Y,Q,N):
      S = 0
      r = yhat-Y
      if np.size(Q) == 1:
          S = np.sum(r**2)
      else:
          for i in range(N):
              # S += np.dot(np.matmul(r[:,i],Q),r[:,i])
              S += r[:,i]@Q@r[:,i]
      return S, r

  def Hg(J,Q,r,p,N):
      H = np.zeros((p,p))
      g = np.zeros(p)
      for i in range(N):
          JQ = J[i].T@Q
          H += JQ@J[i]
          g -= JQ@r[:,i]
      return H,g

  def svdsolve(A,b):
      u,s,v = np.linalg.svd(A)
      c = u.T@b
      w = np.linalg.solve(np.diag(s),c)
      x = v.T@w
      return x

#+end_src
